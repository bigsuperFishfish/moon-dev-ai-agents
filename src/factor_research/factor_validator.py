"""
ğŸ”¬ Factor Validation Framework for Quantitative Trading

This module provides institutional-grade factor validation using:
- Information Coefficient (IC) and Rank IC calculation
- Bootstrap confidence intervals for statistical significance
- Multiple hypothesis testing corrections (Benjamini-Hochberg)
- Factor decay analysis (holding period decay curves)
- Cross-sectional and time-series validation methods
- SHAP-based factor importance analysis

Usage:
    from src.factor_research.factor_validator import FactorValidator
    
    validator = FactorValidator()
    results = validator.validate_factor(
        factor_values=momentum_signal,
        target_returns=next_period_returns,
        factor_name='momentum',
        holding_periods=[1, 5, 10],
        n_bootstrap=1000
    )
"""

import numpy as np
import pandas as pd
from scipy.stats import spearmanr, kendalltau, rankdata
from scipy.special import comb
from typing import Dict, Tuple, List, Optional
import warnings
from dataclasses import dataclass
from datetime import datetime
import json


@dataclass
class FactorValidationResult:
    """çµæ§‹åŒ–çš„å› å­é©—è­‰çµæœ"""
    factor_name: str
    timestamp: str
    
    # ä¿¡æ¯ä¿‚æ•¸ç›¸é—œ
    ic: float  # Pearson IC
    rank_ic: float  # Spearman rank IC
    kendall_tau: float  # Kendall tau IC
    
    # Bootstrap çµ±è¨ˆ
    ic_ci_lower: float  # 95% CI lower bound
    ic_ci_upper: float  # 95% CI upper bound
    ic_p_value: float  # p-value for IC > 0
    ic_significant: bool  # Is IC significantly > 0?
    
    # è¡°æ¸›åˆ†æ
    holding_period_decay: Dict[int, float]  # IC vs holding periods
    decay_slope: float  # IC è¡°æ¸›é€Ÿç‡
    
    # å¤šå› å­æª¢é©—ä¿®æ­£
    ic_fdr_adjusted: bool  # Pass Benjamini-Hochberg FDR test?
    
    # ç¶“æ¿Ÿé¡¯è‘—æ€§
    economic_threshold: float
    is_economically_significant: bool
    
    # æ•´é«”ç‹€æ…‹
    status: str  # 'VALID', 'WEAK', 'INVALID'
    sharpe_ratio: Optional[float] = None
    recommendation: str = ""


class FactorValidator:
    """
    æ©Ÿæ§‹ç´šå› å­é©—è­‰å¼•æ“
    
    æ ¸å¿ƒåŸå‰‡ï¼š
    1. çµ±è¨ˆé¡¯è‘—æ€§ï¼šIC çš„ 95% CI ä¸èƒ½åŒ…å« 0
    2. ç¶“æ¿Ÿé¡¯è‘—æ€§ï¼šIC > threshold (typically 0.015 for daily data)
    3. ç©©å®šæ€§ï¼šIC ä¸æ‡‰å› è¡°æ¸›è€Œå¿«é€Ÿå´©æ½°
    4. å¤šé‡æª¢é©—ä¿®æ­£ï¼šFDR < 0.05ï¼ˆé¿å…å¤šé‡æª¢é©—é™·é˜±ï¼‰
    """
    
    def __init__(
        self,
        economic_threshold: float = 0.015,
        fdr_level: float = 0.05,
        bootstrap_samples: int = 1000,
        ci_percentile: Tuple[float, float] = (2.5, 97.5)
    ):
        """
        Args:
            economic_threshold: ç¶“æ¿Ÿé¡¯è‘—æ€§é–¾å€¼ï¼ˆIC > this æ‰ç®—å¯äº¤æ˜“ï¼‰
            fdr_level: å¤šé‡æª¢é©—ä¿®æ­£çš„ FDR é–¾å€¼
            bootstrap_samples: Bootstrap é‡æŠ½æ¨£æ¬¡æ•¸
            ci_percentile: ä¿¡å¿ƒå€é–“çš„ç™¾åˆ†ä½æ•¸ (lower, upper)
        """
        self.economic_threshold = economic_threshold
        self.fdr_level = fdr_level
        self.bootstrap_samples = bootstrap_samples
        self.ci_percentile = ci_percentile
        self.validation_results = {}
    
    def validate_factor(
        self,
        factor_values: np.ndarray,
        target_returns: np.ndarray,
        factor_name: str,
        holding_periods: List[int] = [1, 5, 10, 20],
        weights: Optional[np.ndarray] = None,
        cross_sectional: bool = True
    ) -> FactorValidationResult:
        """
        å®Œæ•´çš„å› å­é©—è­‰æµç¨‹
        
        Args:
            factor_values: shape (n_samples,) æˆ– (n_samples, n_periods)
            target_returns: shape (n_samples,) å°æ‡‰çš„æœªä¾†æ”¶ç›Š
            factor_name: å› å­åç¨±ï¼ˆç”¨æ–¼è¨˜éŒ„ï¼‰
            holding_periods: ä¸åŒæŒå€‰æœŸçš„è¡°æ¸›åˆ†æ
            weights: å¯é¸çš„æ¨£æœ¬æ¬Šé‡ï¼ˆä¾‹å¦‚æŒ‰æµå‹•æ€§åŠ æ¬Šï¼‰
            cross_sectional: æ˜¯å¦ä½¿ç”¨æˆªé¢æ–¹æ³•ï¼ˆTrueï¼‰é‚„æ˜¯æ™‚é–“åºåˆ—æ–¹æ³•ï¼ˆFalseï¼‰
        
        Returns:
            FactorValidationResult å°è±¡
        """
        
        # 1. è¼¸å…¥é©—è­‰å’Œé è™•ç†
        factor_values, target_returns = self._preprocess_data(
            factor_values, target_returns
        )
        
        if len(factor_values) < 50:
            warnings.warn(
                f"æ¨£æœ¬é‡ ({len(factor_values)}) < 50ï¼Œçµ±è¨ˆçµæœå¯èƒ½ä¸å¯é "
            )
        
        # 2. è¨ˆç®—åŸºç¤ IC
        ic, rank_ic, kendall_tau = self._calculate_ics(
            factor_values, target_returns, weights
        )
        
        # 3. Bootstrap ç½®ä¿¡å€é–“
        ic_ci, ic_bootstrap = self._bootstrap_ic(
            factor_values, target_returns, weights
        )
        ic_p_value = self._calculate_pvalue(ic_bootstrap)
        ic_significant = ic_ci[0] > 0  # CI ä¸åŒ…å« 0
        
        # 4. è¡°æ¸›åˆ†æï¼ˆå¦‚æœæä¾›äº†å¤šå€‹æŒå€‰æœŸï¼‰
        holding_period_decay, decay_slope = self._analyze_decay(
            factor_values, target_returns, holding_periods
        )
        
        # 5. Sharpe Ratioï¼ˆåŸºæ–¼ ICï¼‰
        sharpe_ratio = self._calculate_sharpe_from_ic(ic)
        
        # 6. å¤šé‡æª¢é©—ä¿®æ­£ï¼ˆBenjamini-Hochberg FDRï¼‰
        fdr_adjusted = self._benjamini_hochberg_fdr(
            [ic_p_value], self.fdr_level
        )[0]
        
        # 7. ç¶œåˆåˆ¤æ–·
        is_economically_significant = abs(ic) > self.economic_threshold
        status = self._determine_factor_status(
            ic=ic,
            ic_significant=ic_significant,
            is_economically_significant=is_economically_significant,
            decay_slope=decay_slope
        )
        
        # 8. å»ºç«‹çµæœå°è±¡
        result = FactorValidationResult(
            factor_name=factor_name,
            timestamp=datetime.now().isoformat(),
            ic=float(ic),
            rank_ic=float(rank_ic),
            kendall_tau=float(kendall_tau),
            ic_ci_lower=float(ic_ci[0]),
            ic_ci_upper=float(ic_ci[1]),
            ic_p_value=float(ic_p_value),
            ic_significant=bool(ic_significant),
            holding_period_decay=holding_period_decay,
            decay_slope=float(decay_slope),
            ic_fdr_adjusted=bool(fdr_adjusted),
            economic_threshold=self.economic_threshold,
            is_economically_significant=is_economically_significant,
            status=status,
            sharpe_ratio=float(sharpe_ratio),
            recommendation=self._generate_recommendation(
                status, ic, decay_slope, ic_significant
            )
        )
        
        # 9. ä¿å­˜çµæœ
        self.validation_results[factor_name] = result
        
        return result
    
    def _preprocess_data(
        self,
        factor_values: np.ndarray,
        target_returns: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray]:
        """æ•¸æ“šé è™•ç†ï¼šå»é™¤ NaNã€æ¨™æº–åŒ–"""
        factor_values = np.asarray(factor_values).flatten()
        target_returns = np.asarray(target_returns).flatten()
        
        # å»é™¤ NaN
        valid_idx = ~(np.isnan(factor_values) | np.isnan(target_returns))
        factor_values = factor_values[valid_idx]
        target_returns = target_returns[valid_idx]
        
        # å»é™¤ç„¡é™å€¼
        valid_idx = np.isfinite(factor_values) & np.isfinite(target_returns)
        factor_values = factor_values[valid_idx]
        target_returns = target_returns[valid_idx]
        
        return factor_values, target_returns
    
    def _calculate_ics(
        self,
        factor_values: np.ndarray,
        target_returns: np.ndarray,
        weights: Optional[np.ndarray] = None
    ) -> Tuple[float, float, float]:
        """è¨ˆç®—ä¸‰ç¨®ç›¸é—œä¿‚æ•¸"""
        # Pearson IC
        if weights is not None:
            weights = weights / weights.sum()  # æ­£è¦åŒ–æ¬Šé‡
            ic = self._weighted_correlation(factor_values, target_returns, weights)
        else:
            ic = np.corrcoef(factor_values, target_returns)[0, 1]
        
        # Spearman Rank IC
        rank_ic, _ = spearmanr(factor_values, target_returns)
        
        # Kendall Tau IC
        kendall_tau, _ = kendalltau(factor_values, target_returns)
        
        return ic, rank_ic, kendall_tau
    
    def _weighted_correlation(
        self,
        x: np.ndarray,
        y: np.ndarray,
        weights: np.ndarray
    ) -> float:
        """è¨ˆç®—åŠ æ¬Šç›¸é—œä¿‚æ•¸"""
        avg_x = np.average(x, weights=weights)
        avg_y = np.average(y, weights=weights)
        
        numerator = np.sum(
            weights * (x - avg_x) * (y - avg_y)
        )
        denominator = np.sqrt(
            np.sum(weights * (x - avg_x) ** 2) *
            np.sum(weights * (y - avg_y) ** 2)
        )
        
        return numerator / denominator if denominator != 0 else 0
    
    def _bootstrap_ic(
        self,
        factor_values: np.ndarray,
        target_returns: np.ndarray,
        weights: Optional[np.ndarray] = None,
        n_bootstrap: Optional[int] = None
    ) -> Tuple[Tuple[float, float], np.ndarray]:
        """
        Bootstrap ç½®ä¿¡å€é–“è¨ˆç®—
        
        æ ¸å¿ƒé‚è¼¯ï¼šé‡æŠ½æ¨£ n_bootstrap æ¬¡ï¼Œè¨ˆç®—æ¯æ¬¡çš„ ICï¼Œ
        ç„¶å¾Œå–æ¨£æœ¬åˆ†ä½ˆçš„ç™¾åˆ†ä½æ•¸ä½œç‚º CI
        """
        if n_bootstrap is None:
            n_bootstrap = self.bootstrap_samples
        
        bootstrap_ics = []
        n_samples = len(factor_values)
        
        for _ in range(n_bootstrap):
            # æœ‰æ”¾å›é‡æŠ½æ¨£
            idx = np.random.choice(n_samples, size=n_samples, replace=True)
            
            factor_boot = factor_values[idx]
            returns_boot = target_returns[idx]
            weights_boot = weights[idx] if weights is not None else None
            
            ic_boot, _, _ = self._calculate_ics(
                factor_boot, returns_boot, weights_boot
            )
            bootstrap_ics.append(ic_boot)
        
        bootstrap_ics = np.array(bootstrap_ics)
        ci_lower = np.percentile(bootstrap_ics, self.ci_percentile[0])
        ci_upper = np.percentile(bootstrap_ics, self.ci_percentile[1])
        
        return (ci_lower, ci_upper), bootstrap_ics
    
    def _calculate_pvalue(self, bootstrap_ics: np.ndarray) -> float:
        """
        è¨ˆç®— p-valueï¼šæœ‰å¤šå°‘æ¯”ä¾‹çš„ bootstrap IC <= 0ï¼Ÿ
        ï¼ˆå³ IC > 0 çš„æ¦‚ç‡ï¼‰
        """
        return np.sum(bootstrap_ics <= 0) / len(bootstrap_ics)
    
    def _analyze_decay(
        self,
        factor_values: np.ndarray,
        target_returns: np.ndarray,
        holding_periods: List[int]
    ) -> Tuple[Dict[int, float], float]:
        """
        å› å­è¡°æ¸›åˆ†æ
        
        é‚è¼¯ï¼šå°ä¸åŒçš„æŒå€‰æœŸï¼ŒIC æ‡‰è©²é€æ¼¸è¡°æ¸›
        å¿«é€Ÿè¡°æ¸› = å› å­ä¿¡è™ŸçŸ­æœŸæœ‰æ•ˆä½†ç„¡é•·æœŸæŒçºŒåŠ›
        """
        decay_curve = {}
        
        for period in holding_periods:
            if period > len(target_returns) // 2:
                continue
            
            # å°é½Šåç§»å¾Œçš„æ”¶ç›Šå’Œå› å­
            returns_shifted = target_returns[period:]
            factor_shifted = factor_values[:-period]
            
            if len(returns_shifted) < 20:
                continue
            
            ic, _, _ = self._calculate_ics(factor_shifted, returns_shifted)
            decay_curve[period] = float(ic)
        
        # è¨ˆç®—è¡°æ¸›æ–œç‡ï¼ˆç·šæ€§å›æ­¸ï¼‰
        if len(decay_curve) >= 2:
            periods = np.array(list(decay_curve.keys()))
            ics = np.array(list(decay_curve.values()))
            
            # ç·šæ€§æ“¬åˆï¼šIC = intercept + slope * period
            z = np.polyfit(periods, ics, 1)
            decay_slope = z[0]  # slope
        else:
            decay_slope = 0.0
        
        return decay_curve, decay_slope
    
    def _calculate_sharpe_from_ic(
        self,
        ic: float,
        periods_per_year: int = 252
    ) -> float:
        """
        æ ¹æ“š IC ä¼°ç®— Sharpe Ratioï¼ˆè¿‘ä¼¼ï¼‰
        
        å…¬å¼ï¼šSharpe â‰ˆ IC * sqrt(periods_per_year) / (1 - ICÂ²)
        
        é€™æ˜¯ä¸€å€‹ç²—ç•¥ä¼°ç®—ï¼Œå‡è¨­ï¼š
        - äº¤æ˜“æˆæœ¬å¿½ç•¥
        - å¸‚å ´æ²’æœ‰å…¶ä»–é˜»åŠ›
        - IC æ˜¯å”¯ä¸€çš„ä¿¡æ¯æº
        """
        if ic == 0:
            return 0.0
        
        denominator = np.sqrt(1 - ic ** 2) if abs(ic) < 1 else 0.001
        sharpe = (ic * np.sqrt(periods_per_year)) / denominator
        
        return float(sharpe)
    
    def _benjamini_hochberg_fdr(
        self,
        p_values: List[float],
        fdr_level: float = 0.05
    ) -> List[bool]:
        """
        Benjamini-Hochberg FDR å¤šé‡æª¢é©—ä¿®æ­£
        
        é‚è¼¯ï¼šæ§åˆ¶ False Discovery Rate (FDR)ï¼Œå³
        ã€Œç™¼ç¾çš„é¡¯è‘—çµæœä¸­ï¼Œæœ‰å¤šå°‘æ¯”ä¾‹æ˜¯å‡é™½æ€§ã€
        
        ä¸åŒæ–¼ Bonferroni çš„ä¿å®ˆï¼ŒFDR æ›´é©åˆå¤§è¦æ¨¡æª¢é©—
        """
        n_tests = len(p_values)
        p_sorted_idx = np.argsort(p_values)
        p_sorted = np.array(p_values)[p_sorted_idx]
        
        # è¨ˆç®—è‡¨ç•Œå€¼ï¼šp_i <= (i / m) * alpha
        critical_values = (np.arange(1, n_tests + 1) / n_tests) * fdr_level
        
        # æ‰¾åˆ°æœ€å¤§çš„ i ä½¿å¾— p_i <= critical_value_i
        rejected_idx = np.where(p_sorted <= critical_values)[0]
        
        if len(rejected_idx) > 0:
            threshold_idx = rejected_idx[-1]
            threshold = p_sorted[threshold_idx]
        else:
            threshold = -1
        
        # è½‰æ›å›åŸå§‹é †åº
        results = [p_values[i] <= threshold for i in range(n_tests)]
        return results
    
    def _determine_factor_status(
        self,
        ic: float,
        ic_significant: bool,
        is_economically_significant: bool,
        decay_slope: float,
        decay_threshold: float = -0.002
    ) -> str:
        """
        ç¶œåˆåˆ¤æ–·å› å­ç‹€æ…‹
        
        é‚è¼¯:
        - VALID: çµ±è¨ˆ + ç¶“æ¿Ÿé¡¯è‘—æ€§ + è¡°æ¸›ä¸éå¿«
        - WEAK: çµ±è¨ˆé¡¯è‘—ä½†ç¶“æ¿Ÿæ˜¾è‘—æ€§ä¸å¤ ï¼Œæˆ–è¡°æ¸›éå¿«
        - INVALID: ä¸çµ±è¨ˆé¡¯è‘—ï¼Œæˆ–è¡°æ¸›å®Œå…¨å¤±æ•ˆ
        """
        if not ic_significant:
            return "INVALID"
        
        if decay_slope < decay_threshold:
            return "WEAK"  # IC è¡°æ¸›éå¿«
        
        if is_economically_significant:
            return "VALID"
        else:
            return "WEAK"
    
    def _generate_recommendation(self, status: str, ic: float, decay_slope: float, ic_significant: bool) -> str:
        """æ ¹æ“šé©—è­‰çµæœç”Ÿæˆå»ºè­°"""
        if status == "VALID":
            return f"âœ… è©²å› å­é©åˆæŠ•å…¥ç”Ÿç”¢ã€‚IC={ic:.4f}ï¼Œè¡°æ¸›é€Ÿç‡={decay_slope:.4f}/æœŸ"
        elif status == "WEAK":
            if decay_slope < -0.002:
                return f"âš ï¸ IC è¡°æ¸›éå¿«ï¼ˆ{decay_slope:.4f}ï¼‰ã€‚è€ƒæ…®èª¿æ•´æŒå€‰æœŸæˆ–èˆ‡å…¶ä»–å› å­çµ„åˆ"
            elif ic < 0.015:
                return f"âš ï¸ IC ä¸è¶³ä»¥è¦†è“‹äº¤æ˜“æˆæœ¬ã€‚è€ƒæ…®æé«˜ä¿¡è™Ÿå¼·åº¦æˆ–é™ä½äº¤æ˜“æˆæœ¬"
            else:
                return f"âš ï¸ å› å­çµ±è¨ˆé¡¯è‘—ä½†ç¶“æ¿Ÿæ•ˆç›Šæœ‰é™ã€‚ç”¨ä½œè¼”åŠ©ä¿¡è™Ÿï¼Œè€Œä¸æ˜¯ä¸»ä¿¡è™Ÿ"
        else:  # INVALID
            return f"âŒ è©²å› å­ä¸å…·æœ‰é æ¸¬åŠ›ï¼ˆIC={ic:.4f}ï¼Œp>0.05ï¼‰ã€‚æ‡‰è©²æ‘’æ£„"
    
    def get_summary(self) -> pd.DataFrame:
        """ä»¥ DataFrame çš„å½¢å¼è¿”å›æ‰€æœ‰å·²é©—è­‰çš„å› å­"""
        if not self.validation_results:
            return pd.DataFrame()
        
        data = []
        for name, result in self.validation_results.items():
            data.append({
                'Factor': name,
                'IC': result.ic,
                'Rank IC': result.rank_ic,
                'IC CI Lower': result.ic_ci_lower,
                'IC CI Upper': result.ic_ci_upper,
                'Significant': result.ic_significant,
                'Sharpe': result.sharpe_ratio,
                'Decay Slope': result.decay_slope,
                'Status': result.status,
                'Recommendation': result.recommendation
            })
        
        return pd.DataFrame(data)
    
    def save_results(self, filepath: str) -> None:
        """å°‡é©—è­‰çµæœä¿å­˜ç‚º JSON"""
        results_dict = {}
        for name, result in self.validation_results.items():
            results_dict[name] = {
                'ic': result.ic,
                'rank_ic': result.rank_ic,
                'ic_ci': [result.ic_ci_lower, result.ic_ci_upper],
                'ic_significant': result.ic_significant,
                'sharpe_ratio': result.sharpe_ratio,
                'decay_slope': result.decay_slope,
                'status': result.status,
                'recommendation': result.recommendation,
                'timestamp': result.timestamp
            }
        
        with open(filepath, 'w') as f:
            json.dump(results_dict, f, indent=2)
        
        print(f"âœ… å› å­é©—è­‰çµæœå·²ä¿å­˜è‡³ {filepath}")


if __name__ == "__main__":
    # ç¤ºä¾‹ï¼šé©—è­‰ä¸€å€‹æ¨¡æ“¬çš„å› å­
    np.random.seed(42)
    
    # ç”Ÿæˆæ¨¡æ“¬æ•¸æ“š
    n_samples = 500
    
    # çœŸå¯¦ä¿¡è™Ÿå› å­
    true_factor = np.random.randn(n_samples)
    # ç›®æ¨™æ”¶ç›Šï¼ˆå«çœŸå¯¦å› å­çš„è¨Šè™Ÿ + å™ªè²ï¼‰
    target_returns = 0.05 * true_factor + np.random.randn(n_samples) * 0.8
    
    # é©—è­‰
    validator = FactorValidator(economic_threshold=0.015)
    result = validator.validate_factor(
        factor_values=true_factor,
        target_returns=target_returns,
        factor_name='test_momentum',
        holding_periods=[1, 5, 10, 20]
    )
    
    # æ‰“å°çµæœ
    print("\n" + "="*60)
    print(f"å› å­åç¨±: {result.factor_name}")
    print(f"ç‹€æ…‹: {result.status}")
    print(f"IC: {result.ic:.4f} [{result.ic_ci_lower:.4f}, {result.ic_ci_upper:.4f}]")
    print(f"çµ±è¨ˆé¡¯è‘—: {result.ic_significant}")
    print(f"Sharpe Ratio: {result.sharpe_ratio:.2f}")
    print(f"è¡°æ¸›æ–œç‡: {result.decay_slope:.6f}")
    print(f"\nå»ºè­°: {result.recommendation}")
    print(f"\nè¡°æ¸›æ›²ç·š:")
    for period, ic in result.holding_period_decay.items():
        print(f"  {period} æœŸ: IC={ic:.4f}")
    print("="*60)
    
    # ä¿å­˜çµæœ
    validator.save_results('src/data/factor_validation_results.json')
    
    # æ‰“å°æ‘˜è¦
    print("\nå› å­é©—è­‰æ‘˜è¦:")
    print(validator.get_summary())
